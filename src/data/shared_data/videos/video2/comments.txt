I donâ€™t believe in this Deep Research thing, I donâ€™t see how it is possible to identify relevant sources for a specific topic (i.e. not everyday small things). None of those works, from any company, there is no magic that exists to know if an article is relevant or not to answer a question, only experience in the field of the question can help with that. And even if you know a topic, itâ€™s only by some clues (ex. basic mistakes) that you discover when reading an article if itâ€™s relevant or not.<br>When fact checking will work (i.e. not a Google search like in Geminiâ€¦), we might get advancements, not before that.
Synthesizing unverifiable claims from blog posts on the Internet... nice. I think what is desperately needed is a graph of &quot;true&quot; claims about reality. A mother of all textbooks. For example, I asked Perplexity&#39;s Deep Research to determine the lifecycle of the strawberry plant. After finding 26 academic sources relating to the strawberry plant but not relevant to the question, the agent decided to just regurgitate the Wikipedia page on strawberry plants. What I would really like to have is the ability to traverse down an epistemic tree of claims about reality and understand how any one node is supported. An LLM claims, &quot;strawberries predominantly reproduce vegetatively through stolons.&quot; Great, how do you know that, LLM? I want to see evidence, not a link to Wikipedia.
Glad to see a similar opinion here
Garbage in, Garbage out. The &quot;Deep Research&quot; tools are not a standalone tool that gives the final answer, they are one step in a process. I&#39;m working on a project where the hard part is removing peer review sources that cite retracted papers.
Am i the only one who loves the intro so much?ðŸ˜­
Julius Caesarius lmao
Pffft. When it comes to staying on the bleeding edge of tech I&#39;d rather come here for your critical evaluations and explanations than depend on ANY other form of research for acquiring the same knowledge. All I need is your video and the raw paper (plus a github if available) and I&#39;m set. BUT I dont actively pursue new knowledge by searching or doing research on new papers. I depend on you for that!  ðŸ˜€ And you do a mighty fine job. I come here every single day.
ThisðŸ‘†
He&#39;s not my only source, but is a great one!
this is why i&#39;m building into my agent framework the ability to use multiple llm&#39;s to do a sort od agentic mixture of experts to do law of averages against bias, hallucinations, alignment and goal faking and (particularly in OpenAI) intentional information omission. the bias, alignment and omissions are in their model cards under &quot;safety alignment&quot;. OpenAI is the master of psychological manipulation to &quot;make it friendly&quot; (to stroke your ego to get you to come back) as well as the 3 methods of lying: 1: the outright lie. 2: the partial or half truth. a lie of omission is still a lie. 3: you tell the truth, the whole truth and nothing But the truth, you just tell it in such a way that your audience believes ONLY you! (the only trusted, knowledgeable voice of authority.)
Smart! Tell me more, im interested.
Thanks for the info... its not very good at instruction following ether or  CoT :D
Can we just create some kind of &quot;contra agent&quot; that is tasked to disprove what the research agent comes up with? Then attach the credibility score to both and run the task until the research agent reaches a value greater than X compared to the contra agent.
Yes, you can do that. Then you can have those two debate each other, to provide additional context that can be added to help them decide which, or if they&#39;re both wrong.
That is actually a great idea.
Yes and this is how I&#39;ve been thinking we can model it: Implement Israel&#39;s 10th man rule.
Its sorta funny to just throw some extremely vague questions at it.  I just said hello and the whole thought process was pretty funny.  It was taking overthinking to the extreme and then put out a report titled &quot;Analysis of Human Communication&quot;.<br>One interesting thing though was in the thought process you could see the system instructions.  I think one thing they could do to improve would be to have an option to either just go through its thought process step by step or work as an assistant where you can provide instructions as it goes.<br>Gemini sorta does that with it letting you edit the research plan and from what I&#39;ve heard OpenAI&#39;s deep research will actually ask for clarification or more information.  <br>That should be easy enough to implement. They could also have it that it provides sources which you can go through and remove certain sources or if you find a source that you like they could have the option to grade a source higher.  Maybe put stars beside each source.  1 star means ignore that source.  5 stars means you want it to put more of an emphasis on that source which again should not be difficult to implement.
